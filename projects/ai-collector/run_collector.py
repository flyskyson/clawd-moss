#!/usr/bin/env python3
# run_collector.py
# AIæŠ€æœ¯åŠ¨æ€æ”¶é›†ç³»ç»Ÿä¸»è„šæœ¬

import sys
import os
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from rss_collector import AITechRSSCollector
from content_processor import AITechContentProcessor
from report_generator import AITechReportGenerator

def run_full_pipeline():
    """è¿è¡Œå®Œæ•´çš„æ”¶é›†å¤„ç†ç®¡é“"""
    print("=" * 70)
    print("ğŸš€ MOSS AIæŠ€æœ¯åŠ¨æ€æ”¶é›†ç³»ç»Ÿ v1.0")
    print("=" * 70)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æ­¥éª¤1: æ”¶é›†RSSæ•°æ®
    print("ğŸ“¡ æ­¥éª¤1: æ”¶é›†RSSæ•°æ®")
    print("-" * 40)
    collector = AITechRSSCollector()
    raw_articles = collector.fetch_all_feeds()
    
    if not raw_articles:
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ–‡ç« ï¼Œæµç¨‹ç»ˆæ­¢")
        return None
    
    # ä¿å­˜åŸå§‹æ•°æ®
    raw_data_file = f"data/raw_articles_{timestamp}.json"
    collector.save_articles(raw_data_file)
    
    print(f"âœ… æ­¥éª¤1å®Œæˆ: æ”¶é›†åˆ° {len(raw_articles)} ç¯‡æ–‡ç« ")
    print()
    
    # æ­¥éª¤2: å¤„ç†å†…å®¹
    print("ğŸ§  æ­¥éª¤2: å¤„ç†å†…å®¹")
    print("-" * 40)
    processor = AITechContentProcessor()
    processed_articles = processor.process_articles(raw_articles)
    
    if not processed_articles:
        print("âŒ æ²¡æœ‰å¤„ç†åçš„æ–‡ç« ï¼Œæµç¨‹ç»ˆæ­¢")
        return None
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    processed_data = {
        'processed_at': datetime.now().isoformat(),
        'article_count': len(processed_articles),
        'articles': processed_articles
    }
    
    processed_data_file = f"data/processed_articles_{timestamp}.json"
    os.makedirs(os.path.dirname(processed_data_file), exist_ok=True)
    
    import json
    with open(processed_data_file, 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ­¥éª¤2å®Œæˆ: å¤„ç†äº† {len(processed_articles)} ç¯‡æ–‡ç« ")
    print()
    
    # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Š")
    print("-" * 40)
    generator = AITechReportGenerator()
    result = generator.generate_and_save(processed_articles)
    
    if not result:
        print("âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        return None
    
    print(f"âœ… æ­¥éª¤3å®Œæˆ: ç”Ÿæˆ {result['article_count']} ç¯‡æ–‡ç« çš„æŠ¥å‘Š")
    print()
    
    # æ€»ç»“
    print("ğŸ¯ æµç¨‹æ€»ç»“")
    print("-" * 40)
    print(f"   å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    print(f"   åŸå§‹æ–‡ç« : {len(raw_articles)} ç¯‡")
    print(f"   å¤„ç†æ–‡ç« : {len(processed_articles)} ç¯‡")
    print(f"   æŠ¥å‘Šæ–‡ä»¶: {result.get('markdown', 'N/A')}")
    print(f"   å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    print()
    
    # è¿”å›ç»“æœ
    return {
        'success': True,
        'timestamp': timestamp,
        'raw_articles': len(raw_articles),
        'processed_articles': len(processed_articles),
        'raw_data_file': raw_data_file,
        'processed_data_file': processed_data_file,
        'report_files': result,
        'execution_time': datetime.now().strftime('%H:%M:%S')
    }

def test_system():
    """æµ‹è¯•ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•AIæŠ€æœ¯åŠ¨æ€æ”¶é›†ç³»ç»Ÿ...")
    print()
    
    # æµ‹è¯•RSSæ”¶é›†å™¨
    print("1. æµ‹è¯•RSSæ”¶é›†å™¨...")
    collector = AITechRSSCollector()
    test_feeds = [feed for feed in collector.feeds if feed.get('enabled', True)][:2]  # åªæµ‹è¯•å‰2ä¸ª
    
    test_articles = []
    for feed in test_feeds:
        articles = collector.fetch_feed(feed)
        test_articles.extend(articles)
    
    print(f"âœ… RSSæ”¶é›†å™¨æµ‹è¯•å®Œæˆ: {len(test_articles)} ç¯‡æ–‡ç« ")
    print()
    
    # æµ‹è¯•å†…å®¹å¤„ç†å™¨
    print("2. æµ‹è¯•å†…å®¹å¤„ç†å™¨...")
    processor = AITechContentProcessor()
    processed = processor.process_articles(test_articles)
    
    print(f"âœ… å†…å®¹å¤„ç†å™¨æµ‹è¯•å®Œæˆ: {len(processed)} ç¯‡æ–‡ç« ")
    print()
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
    print("3. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨...")
    generator = AITechReportGenerator()
    
    if processed:
        report = generator.generate_markdown_report(processed[:3])  # åªæµ‹è¯•3ç¯‡
        print("âœ… æŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•å®Œæˆ")
        print()
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("ğŸ“„ æµ‹è¯•æŠ¥å‘Šé¢„è§ˆ:")
        print("-" * 40)
        lines = report.split('\n')[:10]
        for line in lines:
            print(f"   {line}")
        print("   ...")
        print()
        
        return True
    else:
        print("âŒ æ²¡æœ‰å¤„ç†åçš„æ–‡ç« ï¼ŒæŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥")
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AIæŠ€æœ¯åŠ¨æ€æ”¶é›†ç³»ç»Ÿ')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•ç³»ç»ŸåŠŸèƒ½')
    parser.add_argument('--run', action='store_true', help='è¿è¡Œå®Œæ•´æ”¶é›†æµç¨‹')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•ï¼ˆåªæµ‹è¯•2ä¸ªæºï¼‰')
    
    args = parser.parse_args()
    
    if args.test or args.quick:
        success = test_system()
        if success:
            print("ğŸ‰ ç³»ç»Ÿæµ‹è¯•é€šè¿‡!")
        else:
            print("âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥")
            sys.exit(1)
    
    elif args.run:
        result = run_full_pipeline()
        if result:
            print("ğŸ‰ AIæŠ€æœ¯åŠ¨æ€æ”¶é›†å®Œæˆ!")
            print(f"   æŠ¥å‘Šæ–‡ä»¶: {result['report_files']['markdown']}")
        else:
            print("âŒ æ”¶é›†æµç¨‹å¤±è´¥")
            sys.exit(1)
    
    else:
        # é»˜è®¤è¿è¡Œå®Œæ•´æµç¨‹
        result = run_full_pipeline()
        if result:
            print("ğŸ‰ AIæŠ€æœ¯åŠ¨æ€æ”¶é›†å®Œæˆ!")
        else:
            print("âŒ æ”¶é›†æµç¨‹å¤±è´¥")
            sys.exit(1)

if __name__ == "__main__":
    main()