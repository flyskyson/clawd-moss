#!/usr/bin/env python3
# content_processor.py
# AIæŠ€æœ¯åŠ¨æ€å†…å®¹å¤„ç†å™¨

import json
import re
from datetime import datetime
import os

class AITechContentProcessor:
    """AIæŠ€æœ¯åŠ¨æ€å†…å®¹å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        # AIç›¸å…³å…³é”®è¯
        self.ai_keywords = [
            'AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ',
            'è‡ªç„¶è¯­è¨€å¤„ç†', 'è®¡ç®—æœºè§†è§‰', 'å¼ºåŒ–å­¦ä¹ ', 'å¤§è¯­è¨€æ¨¡å‹',
            'GPT', 'Transformer', 'LLM', 'ç”Ÿæˆå¼AI', 'AIGC',
            'è‡ªåŠ¨é©¾é©¶', 'æœºå™¨äºº', 'æ™ºèƒ½åŠ©æ‰‹', 'AI Agent'
        ]
        
        # åˆ†ç±»å…³é”®è¯
        self.category_keywords = {
            'æŠ€æœ¯çªç ´': ['çªç ´', 'åˆ›æ–°', 'æ–°æŠ€æœ¯', 'æ–°ç®—æ³•', 'SOTA', 'state-of-the-art'],
            'åº”ç”¨æ¡ˆä¾‹': ['åº”ç”¨', 'è½åœ°', 'æ¡ˆä¾‹', 'å®è·µ', 'å•†ç”¨', 'éƒ¨ç½²'],
            'å­¦æœ¯ç ”ç©¶': ['è®ºæ–‡', 'ç ”ç©¶', 'å­¦æœ¯', 'arXiv', 'é¢„å°æœ¬', 'æœŸåˆŠ'],
            'å·¥å…·æ¡†æ¶': ['å·¥å…·', 'æ¡†æ¶', 'åº“', 'å¹³å°', 'ç³»ç»Ÿ', 'å¼€æº'],
            'è¡Œä¸šåŠ¨æ€': ['è¡Œä¸š', 'å¸‚åœº', 'æŠ•èµ„', 'èèµ„', 'åˆä½œ', 'å¹¶è´­']
        }
    
    def load_articles(self, input_file):
        """ä»æ–‡ä»¶åŠ è½½æ–‡ç« """
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“‚ åŠ è½½æ–‡ç« : {len(data.get('articles', []))}ç¯‡")
            return data.get('articles', [])
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ç« å¤±è´¥: {e}")
            return []
    
    def filter_ai_articles(self, articles):
        """è¿‡æ»¤AIç›¸å…³æ–‡ç« """
        print("ğŸ” è¿‡æ»¤AIç›¸å…³æ–‡ç« ...")
        
        filtered_articles = []
        for article in articles:
            # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ¤æ–­
            content = f"{article.get('title', '')} {article.get('summary', '')}"
            content_lower = content.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«AIå…³é”®è¯
            is_ai_related = False
            for keyword in self.ai_keywords:
                if keyword.lower() in content_lower:
                    is_ai_related = True
                    break
            
            if is_ai_related:
                # æ·»åŠ AIç›¸å…³åº¦è¯„åˆ†
                ai_score = self.calculate_ai_score(content)
                article['ai_score'] = ai_score
                filtered_articles.append(article)
        
        print(f"âœ… è¿‡æ»¤å®Œæˆ: {len(filtered_articles)}/{len(articles)} ç¯‡AIç›¸å…³")
        return filtered_articles
    
    def calculate_ai_score(self, content):
        """è®¡ç®—AIç›¸å…³åº¦è¯„åˆ†"""
        content_lower = content.lower()
        score = 0
        
        for keyword in self.ai_keywords:
            if keyword.lower() in content_lower:
                score += 1
        
        # å½’ä¸€åŒ–åˆ°0-10åˆ†
        normalized_score = min(score * 2, 10)
        return normalized_score
    
    def categorize_articles(self, articles):
        """å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»"""
        print("ğŸ·ï¸ å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»...")
        
        for article in articles:
            content = f"{article.get('title', '')} {article.get('summary', '')}"
            content_lower = content.lower()
            
            # åˆå§‹åŒ–åˆ†ç±»
            article['categories'] = []
            
            # æ£€æŸ¥æ¯ä¸ªåˆ†ç±»çš„å…³é”®è¯
            for category, keywords in self.category_keywords.items():
                for keyword in keywords:
                    if keyword.lower() in content_lower:
                        if category not in article['categories']:
                            article['categories'].append(category)
                        break
            
            # å¦‚æœæ²¡æœ‰åˆ†ç±»ï¼Œæ ‡è®°ä¸º"å…¶ä»–"
            if not article['categories']:
                article['categories'] = ['å…¶ä»–']
        
        return articles
    
    def generate_summary(self, text, max_length=200):
        """ç”Ÿæˆæ–‡ç« æ‘˜è¦"""
        if not text:
            return "æš‚æ— æ‘˜è¦"
        
        # æ¸…ç†HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # æˆªå–æŒ‡å®šé•¿åº¦
        if len(clean_text) > max_length:
            # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
            truncated = clean_text[:max_length]
            last_period = truncated.rfind('.')
            last_exclamation = truncated.rfind('!')
            last_question = truncated.rfind('?')
            
            cut_point = max(last_period, last_exclamation, last_question)
            if cut_point > max_length * 0.5:  # ç¡®ä¿æˆªæ–­ç‚¹ä¸è¦å¤ªé å‰
                summary = truncated[:cut_point + 1]
            else:
                summary = truncated + "..."
        else:
            summary = clean_text
        
        return summary
    
    def process_articles(self, articles):
        """å¤„ç†æ–‡ç« ï¼šè¿‡æ»¤ã€åˆ†ç±»ã€ç”Ÿæˆæ‘˜è¦"""
        print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ç« ...")
        
        # 1. è¿‡æ»¤AIç›¸å…³æ–‡ç« 
        ai_articles = self.filter_ai_articles(articles)
        
        # 2. å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»
        categorized_articles = self.categorize_articles(ai_articles)
        
        # 3. ç”Ÿæˆæ›´å¥½çš„æ‘˜è¦
        processed_articles = []
        for article in categorized_articles:
            # ä½¿ç”¨æ‘˜è¦æˆ–å†…å®¹ç”Ÿæˆæ›´å¥½çš„æ‘˜è¦
            raw_summary = article.get('summary', '') or article.get('content', '')
            better_summary = self.generate_summary(raw_summary, 150)
            article['processed_summary'] = better_summary
            
            processed_articles.append(article)
        
        print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_articles)}ç¯‡æ–‡ç« ")
        return processed_articles
    
    def get_processing_statistics(self, articles):
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        if not articles:
            return {"total": 0}
        
        stats = {
            'total_processed': len(articles),
            'category_distribution': {},
            'ai_score_distribution': {'high': 0, 'medium': 0, 'low': 0}
        }
        
        for article in articles:
            # åˆ†ç±»åˆ†å¸ƒ
            for category in article.get('categories', []):
                stats['category_distribution'][category] = stats['category_distribution'].get(category, 0) + 1
            
            # AIè¯„åˆ†åˆ†å¸ƒ
            ai_score = article.get('ai_score', 0)
            if ai_score >= 7:
                stats['ai_score_distribution']['high'] += 1
            elif ai_score >= 4:
                stats['ai_score_distribution']['medium'] += 1
            else:
                stats['ai_score_distribution']['low'] += 1
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§  AIæŠ€æœ¯åŠ¨æ€å†…å®¹å¤„ç†å™¨ v1.0")
    print("=" * 60)
    
    # æµ‹è¯•æ•°æ®æ–‡ä»¶
    test_file = "../data/ai_articles_test.json"
    
    if not os.path.exists(test_file):
        print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        print("è¯·å…ˆè¿è¡Œ rss_collector.py æ”¶é›†æ•°æ®")
        return
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = AITechContentProcessor()
    
    # åŠ è½½æ–‡ç« 
    articles = processor.load_articles(test_file)
    
    if articles:
        # å¤„ç†æ–‡ç« 
        processed_articles = processor.process_articles(articles)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_processing_statistics(processed_articles)
        print("\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   å¤„ç†æ–‡ç« æ•°: {stats['total_processed']}")
        print(f"   åˆ†ç±»åˆ†å¸ƒ: {stats['category_distribution']}")
        print(f"   AIè¯„åˆ†åˆ†å¸ƒ: {stats['ai_score_distribution']}")
        
        # æ˜¾ç¤ºå¤„ç†åçš„æ–‡ç« ç¤ºä¾‹
        print("\nğŸ“ å¤„ç†åæ–‡ç« ç¤ºä¾‹:")
        for i, article in enumerate(processed_articles[:2], 1):
            print(f"   {i}. {article['title'][:40]}...")
            print(f"      åˆ†ç±»: {', '.join(article['categories'])}")
            print(f"      AIè¯„åˆ†: {article.get('ai_score', 0)}/10")
            print(f"      æ‘˜è¦: {article['processed_summary'][:60]}...")
            print()
        
        print(f"âœ… å¤„ç†å®Œæˆ!")
        return processed_articles
    else:
        print("âŒ æ²¡æœ‰æ–‡ç« å¯å¤„ç†")
        return []

if __name__ == "__main__":
    main()