#!/usr/bin/env python3
# rss_collector.py
# AIæŠ€æœ¯åŠ¨æ€RSSæ”¶é›†å™¨

import feedparser
import time
from datetime import datetime
import json
import os

class AITechRSSCollector:
    """AIæŠ€æœ¯åŠ¨æ€RSSæ”¶é›†å™¨"""
    
    def __init__(self, config_file=None):
        """åˆå§‹åŒ–æ”¶é›†å™¨"""
        self.feeds = self.load_feeds(config_file)
        self.articles = []
        
    def load_feeds(self, config_file=None):
        """åŠ è½½RSSæºé…ç½®"""
        # é»˜è®¤çš„AIæŠ€æœ¯RSSæº
        default_feeds = [
            {
                'name': 'MIT Technology Review AI',
                'url': 'https://www.technologyreview.com/topic/artificial-intelligence/feed/',
                'category': 'æŠ€æœ¯åª’ä½“',
                'enabled': True
            },
            {
                'name': 'AI Trends',
                'url': 'https://aitrends.com/feed/',
                'category': 'ä¸“ä¸šåª’ä½“',
                'enabled': True
            },
            {
                'name': 'The Batch by deeplearning.ai',
                'url': 'https://www.deeplearning.ai/the-batch/feed/',
                'category': 'æ•™è‚²æœºæ„',
                'enabled': True
            },
            {
                'name': 'OpenAI Blog',
                'url': 'https://openai.com/blog/rss/',
                'category': 'å…¬å¸åšå®¢',
                'enabled': True
            },
            {
                'name': 'Google AI Blog',
                'url': 'https://ai.googleblog.com/feeds/posts/default',
                'category': 'å…¬å¸åšå®¢',
                'enabled': True
            }
        ]
        
        # å¦‚æœæœ‰é…ç½®æ–‡ä»¶ï¼Œä»æ–‡ä»¶åŠ è½½
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    return config.get('feeds', default_feeds)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é…ç½®")
        
        return default_feeds
    
    def fetch_feed(self, feed_config):
        """è·å–å•ä¸ªRSSæºçš„å†…å®¹"""
        try:
            print(f"ğŸ“¡ æ­£åœ¨è·å–: {feed_config['name']}...")
            feed = feedparser.parse(feed_config['url'])
            
            if feed.bozo:
                print(f"âš ï¸ è§£æRSSå¤±è´¥: {feed.bozo_exception}")
                return []
            
            articles = []
            for entry in feed.entries[:10]:  # æ¯ä¸ªæºæœ€å¤šå–10æ¡
                # æå–æ–‡ç« ä¿¡æ¯
                article = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'link': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', ''),
                    'content': entry.get('content', [{}])[0].get('value', '') if entry.get('content') else '',
                    'source': feed_config['name'],
                    'category': feed_config['category'],
                    'feed_url': feed_config['url'],
                    'collected_at': datetime.now().isoformat()
                }
                articles.append(article)
            
            print(f"âœ… è·å–æˆåŠŸ: {feed_config['name']} - {len(articles)}ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥ {feed_config['name']}: {e}")
            return []
    
    def fetch_all_feeds(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„RSSæº"""
        print("ğŸš€ å¼€å§‹è·å–AIæŠ€æœ¯åŠ¨æ€...")
        print(f"ğŸ“Š é…ç½®äº† {len(self.feeds)} ä¸ªRSSæº")
        
        all_articles = []
        enabled_count = 0
        
        for feed in self.feeds:
            if not feed.get('enabled', True):
                continue
                
            enabled_count += 1
            articles = self.fetch_feed(feed)
            all_articles.extend(articles)
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        print(f"ğŸ¯ å®Œæˆè·å–: {enabled_count}ä¸ªæº, å…±{len(all_articles)}ç¯‡æ–‡ç« ")
        self.articles = all_articles
        return all_articles
    
    def save_articles(self, output_file=None):
        """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
        if not self.articles:
            print("âš ï¸ æ²¡æœ‰æ–‡ç« å¯ä¿å­˜")
            return None
        
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"../data/ai_articles_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'collected_at': datetime.now().isoformat(),
                    'article_count': len(self.articles),
                    'articles': self.articles
                }, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æ–‡ç« å·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return None
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.articles:
            return {"total": 0}
        
        stats = {
            'total': len(self.articles),
            'by_source': {},
            'by_category': {}
        }
        
        for article in self.articles:
            # æŒ‰æ¥æºç»Ÿè®¡
            source = article['source']
            stats['by_source'][source] = stats['by_source'].get(source, 0) + 1
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            category = article['category']
            stats['by_category'][category] = stats['by_category'].get(category, 0) + 1
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– AIæŠ€æœ¯åŠ¨æ€RSSæ”¶é›†å™¨ v1.0")
    print("=" * 60)
    
    # åˆ›å»ºæ”¶é›†å™¨
    collector = AITechRSSCollector()
    
    # è·å–æ‰€æœ‰RSSæº
    articles = collector.fetch_all_feeds()
    
    if articles:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = collector.get_statistics()
        print("\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ç« æ•°: {stats['total']}")
        print(f"   æ¥æºåˆ†å¸ƒ: {stats['by_source']}")
        print(f"   åˆ†ç±»åˆ†å¸ƒ: {stats['by_category']}")
        
        # ä¿å­˜æ–‡ç« 
        output_file = collector.save_articles()
        
        # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
        print("\nğŸ“° æœ€æ–°æ–‡ç« é¢„è§ˆ:")
        for i, article in enumerate(articles[:3], 1):
            print(f"   {i}. {article['title'][:50]}...")
            print(f"      æ¥æº: {article['source']}")
            print(f"      é“¾æ¥: {article['link'][:50]}...")
            print()
        
        print(f"âœ… æ”¶é›†å®Œæˆ! å…±{len(articles)}ç¯‡æ–‡ç« ")
        return output_file
    else:
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ–‡ç« ")
        return None

if __name__ == "__main__":
    main()